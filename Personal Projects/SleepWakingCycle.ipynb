{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import re\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import silhouette_score, confusion_matrix, f1_score, classification_report, accuracy_score\n",
    "from scipy.stats import kruskal, f_oneway\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, scale\n",
    "from sklearn import (manifold, datasets, decomposition, ensemble,\n",
    "                     discriminant_analysis, random_projection, neighbors, metrics)\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.feature_selection import (f_classif, VarianceThreshold, chi2, mutual_info_classif, SelectKBest,\n",
    "                                       SelectPercentile, SelectFromModel)\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: /Users/sbk/Documents/Datasets/SleepPredictions/child-mind-institute-detect-sleep-states/train_series.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/dask/backends.py:136\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    137\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/dask/dataframe/io/parquet/core.py:543\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, columns, filters, categories, index, storage_options, engine, use_nullable_dtypes, dtype_backend, calculate_divisions, ignore_metadata_file, metadata_task_size, split_row_groups, blocksize, aggregate_files, parquet_file_extension, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m     blocksize \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m read_metadata_result \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39;49mread_metadata(\n\u001b[1;32m    544\u001b[0m     fs,\n\u001b[1;32m    545\u001b[0m     paths,\n\u001b[1;32m    546\u001b[0m     categories\u001b[39m=\u001b[39;49mcategories,\n\u001b[1;32m    547\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m    548\u001b[0m     use_nullable_dtypes\u001b[39m=\u001b[39;49muse_nullable_dtypes,\n\u001b[1;32m    549\u001b[0m     dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,\n\u001b[1;32m    550\u001b[0m     gather_statistics\u001b[39m=\u001b[39;49mcalculate_divisions,\n\u001b[1;32m    551\u001b[0m     filters\u001b[39m=\u001b[39;49mfilters,\n\u001b[1;32m    552\u001b[0m     split_row_groups\u001b[39m=\u001b[39;49msplit_row_groups,\n\u001b[1;32m    553\u001b[0m     blocksize\u001b[39m=\u001b[39;49mblocksize,\n\u001b[1;32m    554\u001b[0m     aggregate_files\u001b[39m=\u001b[39;49maggregate_files,\n\u001b[1;32m    555\u001b[0m     ignore_metadata_file\u001b[39m=\u001b[39;49mignore_metadata_file,\n\u001b[1;32m    556\u001b[0m     metadata_task_size\u001b[39m=\u001b[39;49mmetadata_task_size,\n\u001b[1;32m    557\u001b[0m     parquet_file_extension\u001b[39m=\u001b[39;49mparquet_file_extension,\n\u001b[1;32m    558\u001b[0m     dataset\u001b[39m=\u001b[39;49mdataset_options,\n\u001b[1;32m    559\u001b[0m     read\u001b[39m=\u001b[39;49mread_options,\n\u001b[1;32m    560\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mother_options,\n\u001b[1;32m    561\u001b[0m )\n\u001b[1;32m    563\u001b[0m \u001b[39m# In the future, we may want to give the engine the\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# option to return a dedicated element for `common_kwargs`.\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39m# However, to avoid breaking the API, we just embed this\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39m# data in the first element of `parts` for now.\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[39m# The logic below is inteded to handle backward and forward\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[39m# compatibility with a user-defined engine.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/dask/dataframe/io/parquet/arrow.py:532\u001b[0m, in \u001b[0;36mArrowDatasetEngine.read_metadata\u001b[0;34m(cls, fs, paths, categories, index, use_nullable_dtypes, dtype_backend, gather_statistics, filters, split_row_groups, blocksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[39m# Stage 1: Collect general dataset information\u001b[39;00m\n\u001b[0;32m--> 532\u001b[0m dataset_info \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_collect_dataset_info(\n\u001b[1;32m    533\u001b[0m     paths,\n\u001b[1;32m    534\u001b[0m     fs,\n\u001b[1;32m    535\u001b[0m     categories,\n\u001b[1;32m    536\u001b[0m     index,\n\u001b[1;32m    537\u001b[0m     gather_statistics,\n\u001b[1;32m    538\u001b[0m     filters,\n\u001b[1;32m    539\u001b[0m     split_row_groups,\n\u001b[1;32m    540\u001b[0m     blocksize,\n\u001b[1;32m    541\u001b[0m     aggregate_files,\n\u001b[1;32m    542\u001b[0m     ignore_metadata_file,\n\u001b[1;32m    543\u001b[0m     metadata_task_size,\n\u001b[1;32m    544\u001b[0m     parquet_file_extension,\n\u001b[1;32m    545\u001b[0m     kwargs,\n\u001b[1;32m    546\u001b[0m )\n\u001b[1;32m    548\u001b[0m \u001b[39m# Stage 2: Generate output `meta`\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/dask/dataframe/io/parquet/arrow.py:1047\u001b[0m, in \u001b[0;36mArrowDatasetEngine._collect_dataset_info\u001b[0;34m(cls, paths, fs, categories, index, gather_statistics, filters, split_row_groups, blocksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, kwargs)\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[39mif\u001b[39;00m ds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1047\u001b[0m     ds \u001b[39m=\u001b[39m pa_ds\u001b[39m.\u001b[39;49mdataset(\n\u001b[1;32m   1048\u001b[0m         paths,\n\u001b[1;32m   1049\u001b[0m         filesystem\u001b[39m=\u001b[39;49m_wrapped_fs(fs),\n\u001b[1;32m   1050\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_processed_dataset_kwargs,\n\u001b[1;32m   1051\u001b[0m     )\n\u001b[1;32m   1053\u001b[0m \u001b[39m# Get file_frag sample and extract physical_schema\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyarrow/dataset.py:776\u001b[0m, in \u001b[0;36mdataset\u001b[0;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(_is_path_like(elem) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m source):\n\u001b[0;32m--> 776\u001b[0m     \u001b[39mreturn\u001b[39;00m _filesystem_dataset(source, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    777\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(elem, Dataset) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m source):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyarrow/dataset.py:454\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[0;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(source, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 454\u001b[0m     fs, paths_or_selector \u001b[39m=\u001b[39m _ensure_multiple_sources(source, filesystem)\n\u001b[1;32m    455\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyarrow/dataset.py:373\u001b[0m, in \u001b[0;36m_ensure_multiple_sources\u001b[0;34m(paths, filesystem)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39melif\u001b[39;00m file_type \u001b[39m==\u001b[39m FileType\u001b[39m.\u001b[39mNotFound:\n\u001b[0;32m--> 373\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(info\u001b[39m.\u001b[39mpath)\n\u001b[1;32m    374\u001b[0m \u001b[39melif\u001b[39;00m file_type \u001b[39m==\u001b[39m FileType\u001b[39m.\u001b[39mDirectory:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: /Users/sbk/Documents/Datasets/SleepPredictions/child-mind-institute-detect-sleep-states/train_series.parquet",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/sbk/Documents/GitHub/Barryjuait/Personal Projects/SleepWakingCycle.ipynb Cell 2\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sbk/Documents/GitHub/Barryjuait/Personal%20Projects/SleepWakingCycle.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Load datasets\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sbk/Documents/GitHub/Barryjuait/Personal%20Projects/SleepWakingCycle.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_events \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(homeFolder \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain_events.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sbk/Documents/GitHub/Barryjuait/Personal%20Projects/SleepWakingCycle.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_series \u001b[39m=\u001b[39m dd\u001b[39m.\u001b[39;49mread_parquet(homeFolder \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mtrain_series.parquet\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/dask/backends.py:138\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    137\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mtype\u001b[39m(e)(\n\u001b[1;32m    139\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling the \u001b[39m\u001b[39m{\u001b[39;00mfuncname(func)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmethod registered to the \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackend\u001b[39m}\u001b[39;00m\u001b[39m backend.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOriginal Message: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: /Users/sbk/Documents/Datasets/SleepPredictions/child-mind-institute-detect-sleep-states/train_series.parquet"
     ]
    }
   ],
   "source": [
    "homeFolder = '/Users/sbk/Documents/Datasets/SleepPredictions/child-mind-institute-detect-sleep-states/'\n",
    "\n",
    "# Load datasets\n",
    "train_events = pd.read_csv(homeFolder + 'train_events.csv')\n",
    "train_series = dd.read_parquet(homeFolder + 'train_series.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>night</th>\n",
       "      <th>event</th>\n",
       "      <th>step</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>1</td>\n",
       "      <td>onset</td>\n",
       "      <td>4992.0</td>\n",
       "      <td>2018-08-14T22:26:00-0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>1</td>\n",
       "      <td>wakeup</td>\n",
       "      <td>10932.0</td>\n",
       "      <td>2018-08-15T06:41:00-0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>2</td>\n",
       "      <td>onset</td>\n",
       "      <td>20244.0</td>\n",
       "      <td>2018-08-15T19:37:00-0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>2</td>\n",
       "      <td>wakeup</td>\n",
       "      <td>27492.0</td>\n",
       "      <td>2018-08-16T05:41:00-0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>3</td>\n",
       "      <td>onset</td>\n",
       "      <td>39996.0</td>\n",
       "      <td>2018-08-16T23:03:00-0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>3</td>\n",
       "      <td>wakeup</td>\n",
       "      <td>44400.0</td>\n",
       "      <td>2018-08-17T05:10:00-0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>4</td>\n",
       "      <td>onset</td>\n",
       "      <td>57240.0</td>\n",
       "      <td>2018-08-17T23:00:00-0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>4</td>\n",
       "      <td>wakeup</td>\n",
       "      <td>62856.0</td>\n",
       "      <td>2018-08-18T06:48:00-0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>5</td>\n",
       "      <td>onset</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>5</td>\n",
       "      <td>wakeup</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>6</td>\n",
       "      <td>onset</td>\n",
       "      <td>91296.0</td>\n",
       "      <td>2018-08-19T22:18:00-0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>6</td>\n",
       "      <td>wakeup</td>\n",
       "      <td>97860.0</td>\n",
       "      <td>2018-08-20T07:25:00-0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>7</td>\n",
       "      <td>onset</td>\n",
       "      <td>109500.0</td>\n",
       "      <td>2018-08-20T23:35:00-0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>7</td>\n",
       "      <td>wakeup</td>\n",
       "      <td>118524.0</td>\n",
       "      <td>2018-08-21T12:07:00-0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>8</td>\n",
       "      <td>onset</td>\n",
       "      <td>127296.0</td>\n",
       "      <td>2018-08-22T00:18:00-0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>8</td>\n",
       "      <td>wakeup</td>\n",
       "      <td>133332.0</td>\n",
       "      <td>2018-08-22T08:41:00-0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>9</td>\n",
       "      <td>onset</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>9</td>\n",
       "      <td>wakeup</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>10</td>\n",
       "      <td>onset</td>\n",
       "      <td>159972.0</td>\n",
       "      <td>2018-08-23T21:41:00-0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>038441c925bb</td>\n",
       "      <td>10</td>\n",
       "      <td>wakeup</td>\n",
       "      <td>167400.0</td>\n",
       "      <td>2018-08-24T08:00:00-0400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       series_id  night   event      step                 timestamp\n",
       "0   038441c925bb      1   onset    4992.0  2018-08-14T22:26:00-0400\n",
       "1   038441c925bb      1  wakeup   10932.0  2018-08-15T06:41:00-0400\n",
       "2   038441c925bb      2   onset   20244.0  2018-08-15T19:37:00-0400\n",
       "3   038441c925bb      2  wakeup   27492.0  2018-08-16T05:41:00-0400\n",
       "4   038441c925bb      3   onset   39996.0  2018-08-16T23:03:00-0400\n",
       "5   038441c925bb      3  wakeup   44400.0  2018-08-17T05:10:00-0400\n",
       "6   038441c925bb      4   onset   57240.0  2018-08-17T23:00:00-0400\n",
       "7   038441c925bb      4  wakeup   62856.0  2018-08-18T06:48:00-0400\n",
       "8   038441c925bb      5   onset       NaN                       NaN\n",
       "9   038441c925bb      5  wakeup       NaN                       NaN\n",
       "10  038441c925bb      6   onset   91296.0  2018-08-19T22:18:00-0400\n",
       "11  038441c925bb      6  wakeup   97860.0  2018-08-20T07:25:00-0400\n",
       "12  038441c925bb      7   onset  109500.0  2018-08-20T23:35:00-0400\n",
       "13  038441c925bb      7  wakeup  118524.0  2018-08-21T12:07:00-0400\n",
       "14  038441c925bb      8   onset  127296.0  2018-08-22T00:18:00-0400\n",
       "15  038441c925bb      8  wakeup  133332.0  2018-08-22T08:41:00-0400\n",
       "16  038441c925bb      9   onset       NaN                       NaN\n",
       "17  038441c925bb      9  wakeup       NaN                       NaN\n",
       "18  038441c925bb     10   onset  159972.0  2018-08-23T21:41:00-0400\n",
       "19  038441c925bb     10  wakeup  167400.0  2018-08-24T08:00:00-0400"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_events.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_series' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/sbk/Documents/GitHub/Barryjuait/Personal Projects/SleepWakingCycle.ipynb Cell 4\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sbk/Documents/GitHub/Barryjuait/Personal%20Projects/SleepWakingCycle.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_series\u001b[39m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_series' is not defined"
     ]
    }
   ],
   "source": [
    "train_series.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dff = train_series.merge(train_events, on='series_id', how='inner')\n",
    "\n",
    "# Compute the result to get a Pandas DataFrame\n",
    "dff = dff.compute()\n",
    "\n",
    "# Reset the index and describe the DataFrame\n",
    "dff.reset_index(drop=True, inplace=True)\n",
    "description = dff.describe().T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
